{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We check the speed differences between single/multicore CPU and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "if not torch.cuda.is_available():\n",
    "    print 'This tutorial expect a GPU'\n",
    "else:\n",
    "    torch.cuda.set_device(0)\n",
    "    num_gpus    = torch.cuda.device_count()\n",
    "    current_gpu = torch.cuda.device_count()\n",
    "    print \"Current device index: {}. Total number of devices: {}\".format(current_gpu,num_gpus)\n",
    "    print torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronize\n",
    "CUDA operation are performed asynchronous, i.e. the phython code may continue before the operation is finished. By torch.cuda.synchronize() the program flow is forced to wait for completion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple jupyter %timeit works but it may choose a too large number of loops\n",
    "# timeit adapts the number of tests(loops) to the speed of the operation.\n",
    "# Without torch.cuda.synchronize() the operation looks quicker then it is\n",
    "# This example may then run a few minutes\n",
    "size=8192\n",
    "A=torch.randn(size,size)\n",
    "B=torch.randn(size,size)\n",
    "%timeit A.mm(B)\n",
    "\n",
    "A=A.cuda()\n",
    "B=B.cuda()\n",
    "%timeit A.mm(B);torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# timing of cell behaves similar\n",
    "A.mm(B)\n",
    "torch.cuda.synchronize()\n",
    "# w/o torch.cuda.synchronize(): 1000 loops, best of 3: 121 ms per loop\n",
    "# with torch.cuda.synchronize():   1 loop, best of 3: 121 ms per loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If yo run this on a multicore machine you can benefit from multi-threading MKL\n",
    "\n",
    "E.g. on a machine with 2 Intel Xeon E5‚Äê2600v4, i.e. 20 cores, \n",
    "\n",
    "* numpy\n",
    "\n",
    " * 1 threads</br>\n",
    " Dotted two 4096x4096 matrices in 3.45 s.</br>\n",
    " Dotted two vectors of length 524288 in 0.23 ms.</br>\n",
    " SVD of a 2048x1024 matrix in 1.49 s.</br>\n",
    " Eigendecomposition of a 2048x2048 matrix in 8.95 s.</br>\n",
    "\n",
    " * 40 threads</br>\n",
    " Dotted two 4096x4096 matrices in 0.29 s.</br>\n",
    " Dotted two vectors of length 524288 in 0.03 ms.</br>\n",
    " SVD of a 2048x1024 matrix in 0.42 s.</br>\n",
    " Eigendecomposition of a 2048x2048 matrix in 3.93 s.</br>\n",
    "\n",
    "* torch - cpu\n",
    " * 1 thread</br>\n",
    " Dotted two 4096x4096 matrices in 1.74 s.</br>\n",
    " Dotted two vectors of length 524288 in 0.12 ms.</br>\n",
    " SVD of a 2048x1024 matrix in 2.81 s.</br>\n",
    " Eigendecomposition of a 2048x2048 matrix in 2.15 s.</br>\n",
    " \n",
    " * 40 threads</br>\n",
    " Dotted two 4096x4096 matrices in 0.13 s.</br>\n",
    " Dotted two vectors of length 524288 in 0.01 ms.</br>\n",
    " SVD of a 2048x1024 matrix in 0.81 s.</br>\n",
    " Eigendecomposition of a 2048x2048 matrix in 1.49 s.</br>\n",
    "\n",
    "* torch - cuda float32\n",
    " * Tesla P100-PCIE-16GB</br>\n",
    " Dotted two 4096x4096 matrices in 0.02 s.</br>\n",
    " Dotted two vectors of length 524288 in 0.05 ms.</br>\n",
    " SVD of a 2048x1024 matrix in 0.32 s.</br>\n",
    " Eigendecomposition of a 2048x2048 matrix in 1.40 s.</br></br>\n",
    " Dotted two 8192x8192 matrices in 0.12 s.</br>\n",
    " Dotted two vectors of length 1048576 in 0.06 ms.</br>\n",
    " SVD of a 4096x2048 matrix in 1.40 s.</br>\n",
    " Eigendecomposition of a 4096x4096 matrix in 6.86 s.</br>\n",
    " * torch - cuda float64</br>\n",
    " Dotted two 8192x8192 matrices in 0.24 s.</br>\n",
    " Dotted two vectors of length 1048576 in 0.07 ms.</br>\n",
    " SVD of a 4096x2048 matrix in 2.36 s.</br>\n",
    " Eigendecomposition of a 4096x4096 matrix in 12.61 s.</br>\n",
    "\n",
    "Get the same numbers for your AWS machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic linear algebra operation with numpy\n",
    "import mkl\n",
    "mkl.set_num_threads(1)\n",
    "#mkl.set_num_threads(40)  # run this if you have more cores available for yourself\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "# Let's take the randomness out of random numbers (for reproducibility)\n",
    "np.random.seed(0)\n",
    "\n",
    "size = 4096\n",
    "A, B = np.random.random((size, size)), np.random.random((size, size))\n",
    "C, D = np.random.random((size * 128,)), np.random.random((size * 128,))\n",
    "E = np.random.random((int(size / 2), int(size / 4)))\n",
    "F = np.random.random((int(size / 2), int(size / 2)))\n",
    "F = np.dot(F, F.T)\n",
    "G = np.random.random((int(size / 2), int(size / 2)))\n",
    "\n",
    "# Matrix multiplication\n",
    "N = 20\n",
    "t = time()\n",
    "for i in range(N):\n",
    "    np.dot(A, B)\n",
    "delta = time() - t\n",
    "print 'Dotted two %dx%d matrices in %0.2f s.' % (size, size, delta / N)\n",
    "del A, B\n",
    "\n",
    "# Vector multiplication\n",
    "N = 5000\n",
    "t = time()\n",
    "for i in range(N):\n",
    "    np.dot(C, D)\n",
    "delta = time() - t\n",
    "print 'Dotted two vectors of length %d in %0.2f ms.' % (size * 128, 1e3 * delta / N)\n",
    "del C, D\n",
    "\n",
    "# Singular Value Decomposition (SVD)\n",
    "N = 3\n",
    "t = time()\n",
    "for i in range(N):\n",
    "    np.linalg.svd(E, full_matrices = True)\n",
    "delta = time() - t\n",
    "print \"SVD of a %dx%d matrix in %0.2f s.\" % (size / 2, size / 4, delta / N)\n",
    "del E\n",
    "\n",
    "# Cholesky Decomposition\n",
    "N = 3\n",
    "t = time()\n",
    "for i in range(N):\n",
    "    np.linalg.cholesky(F)\n",
    "delta = time() - t\n",
    "print(\"Cholesky decomposition of a %dx%d matrix in %0.2f s.\" % (size / 2, size / 2, delta / N))\n",
    "\n",
    "# Eigendecomposition\n",
    "t = time()\n",
    "for i in range(N):\n",
    "    np.linalg.eig(G)\n",
    "delta = time() - t\n",
    "print \"Eigendecomposition of a %dx%d matrix in %0.2f s.\" % (size / 2, size / 2, delta / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic linear algebra operation with pytorch CPU\n",
    "import mkl\n",
    "mkl.set_num_threads(1)\n",
    "#mkl.set_num_threads(40)\n",
    "import torch\n",
    "from time import time\n",
    "\n",
    "# Let's take the randomness out of random numbers (for reproducibility)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "size = 4096\n",
    "A, B = torch.rand((size, size)), torch.rand((size, size))\n",
    "C, D = torch.rand((size * 128,)), torch.rand((size * 128,))\n",
    "E = torch.rand((int(size / 2), int(size / 4)))\n",
    "F = torch.rand((int(size / 2), int(size / 2)))\n",
    "F = torch.mm(F, F.t())\n",
    "G = torch.rand((int(size / 2), int(size / 2)))\n",
    "\n",
    "# Matrix multiplication\n",
    "N = 20\n",
    "t = time()\n",
    "for i in range(N):\n",
    "    torch.mm(A, B)\n",
    "delta = time() - t\n",
    "print 'Dotted two %dx%d matrices in %0.2f s.' % (size, size, delta / N)\n",
    "del A, B\n",
    "\n",
    "# Vector multiplication\n",
    "N = 5000\n",
    "t = time()\n",
    "for i in range(N):\n",
    "    torch.dot(C, D)\n",
    "delta = time() - t\n",
    "print 'Dotted two vectors of length %d in %0.2f ms.' % (size * 128, 1e3 * delta / N)\n",
    "del C, D\n",
    "\n",
    "# Singular Value Decomposition (SVD)\n",
    "N = 3\n",
    "t = time()\n",
    "for i in range(N):\n",
    "    torch.svd(E)\n",
    "delta = time() - t\n",
    "print \"SVD of a %dx%d matrix in %0.2f s.\" % (size / 2, size / 4, delta / N)\n",
    "del E\n",
    "\n",
    "\n",
    "# Eigendecomposition\n",
    "t = time()\n",
    "for i in range(N):\n",
    "    torch.eig(G)\n",
    "delta = time() - t\n",
    "print \"Eigendecomposition of a %dx%d matrix in %0.2f s.\" % (size / 2, size / 2, delta / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic linear algebra operation with pytorch CUDA\n",
    "import mkl\n",
    "mkl.set_num_threads(1)\n",
    "import torch\n",
    "from time import time\n",
    "\n",
    "# Let's take the randomness out of random numbers (for reproducibility)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "dtype=torch.float32\n",
    "#dtype=torch.float64\n",
    "#size = 8192\n",
    "size = 4096\n",
    "\n",
    "A, B = torch.rand((size, size),dtype=dtype), torch.rand((size, size),dtype=dtype)\n",
    "C, D = torch.rand((size * 128,),dtype=dtype), torch.rand((size * 128,),dtype=dtype)\n",
    "E = torch.rand((int(size / 2), int(size / 4)),dtype=dtype)\n",
    "F = torch.rand((int(size / 2), int(size / 2)),dtype=dtype)\n",
    "F = torch.mm(F, F.t())\n",
    "G = torch.rand((int(size / 2), int(size / 2)),dtype=dtype)\n",
    "\n",
    "torch.cuda.device(0)\n",
    "print torch.cuda.get_device_name(0)\n",
    "\n",
    "A=A.cuda()\n",
    "B=B.cuda()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "print B.is_cuda, A.is_cuda\n",
    "\n",
    "# Matrix multiplication\n",
    "N = 20\n",
    "t = time()\n",
    "for i in range(N):\n",
    "    torch.mm(A, B)\n",
    "    # cuda operations are asynchronous - synchronize() waits that operation is finished\n",
    "    torch.cuda.synchronize()  \n",
    "delta = time() - t\n",
    "print 'Dotted two %dx%d matrices in %0.2f s.' % (size, size, delta / N)\n",
    "del A, B\n",
    "\n",
    "C=C.cuda()\n",
    "D=D.cuda()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "# Vector multiplication\n",
    "N = 5000\n",
    "t = time()\n",
    "for i in range(N):\n",
    "    torch.dot(C, D)\n",
    "    torch.cuda.synchronize()\n",
    "delta = time() - t\n",
    "print 'Dotted two vectors of length %d in %0.2f ms.' % (size * 128, 1e3 * delta / N)\n",
    "del C, D\n",
    "\n",
    "E=E.cuda()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "# Singular Value Decomposition (SVD)\n",
    "N = 3\n",
    "t = time()\n",
    "for i in range(N):\n",
    "    torch.svd(E)\n",
    "    torch.cuda.synchronize()\n",
    "delta = time() - t\n",
    "print \"SVD of a %dx%d matrix in %0.2f s.\" % (size / 2, size / 4, delta / N)\n",
    "del E\n",
    "\n",
    "G=G.cuda()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Eigendecomposition\n",
    "t = time()\n",
    "for i in range(N):\n",
    "    torch.eig(G)\n",
    "    torch.cuda.synchronize()\n",
    "delta = time() - t\n",
    "print \"Eigendecomposition of a %dx%d matrix in %0.2f s.\" % (size / 2, size / 2, delta / N)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch04]",
   "language": "python",
   "name": "conda-env-torch04-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
